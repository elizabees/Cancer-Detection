Logistic Regression performed the best with 0.98 accuracy for MLE and MAP.  This is most likely due to the data being linearly seperable, which logestic regression is well suited for.  K-nn also performed well with 0.96 accuracy using Manhattan distance.  There was a clear boundary between benign and malignant, which helps k-nn perform well.  Perceptron and SVM did not perform as well as k-nn and logistic regression but still not bad with a 0.94 accuracy for both.  This slightly lower performance may be due to overfitting, especially in response to noise.  Logistic regression was likely less affected by noise due to its probabilistic approach, which creates softer boundaries. k-NNâ€™s lack of a fixed boundary line also helped it handle noise, as individual points are less likely to affect classification.  K means performed the worst of the bunch.  This was mostly expected, due to it being an unsupervised model and not being able to use a training set.